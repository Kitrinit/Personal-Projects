{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0402af91-faec-4ed0-9244-27671d2c2b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080048ea-0965-46d2-917c-720ab7a902f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from playwright.async_api import async_playwright\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4417597-53df-4228-b69a-68dc032803aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_jobstreet():\n",
    "    scrape_start_time = datetime.now()\n",
    "    file_path = \"../../Projects-Data/Job-Scraping/Data.xlsx\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        existing_df = pd.read_excel(file_path)\n",
    "        existing_ids = set(existing_df[\"Job ID\"].dropna().astype(str).unique())\n",
    "    else:\n",
    "        existing_df = pd.DataFrame(columns=[\n",
    "            \"Job Title\", \"Company\", \"Location\", \"Work Type\", \"Classification\",\n",
    "            \"Salary\", \"Job Description\", \"Job ID\", \"Posted Time\"\n",
    "        ])\n",
    "        existing_ids = set()\n",
    "\n",
    "    new_data = []\n",
    "    page_num = 1\n",
    "    stop_scraping = False\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        while not stop_scraping:\n",
    "            url = f\"https://ph.jobstreet.com/Data-Science-jobs?page={page_num}&sortmode=ListedDate\"\n",
    "            print(f\"\\nüåê Navigating to Page {page_num} ‚Äî {url}\")\n",
    "\n",
    "            try:\n",
    "                await page.goto(url, timeout=30000)\n",
    "                await page.wait_for_selector(\"article[data-testid='job-card']\", timeout=5000)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Retry page {page_num} due to error: {e}\")\n",
    "                try:\n",
    "                    await page.goto(url, timeout=30000)\n",
    "                    await page.wait_for_selector(\"article[data-testid='job-card']\", timeout=5000)\n",
    "                except:\n",
    "                    print(f\"‚ö†Ô∏è Skipping page {page_num} ‚Äî no job listings or permanent failure.\")\n",
    "                    page_num += 1\n",
    "                    continue\n",
    "\n",
    "            await asyncio.sleep(2)\n",
    "            jobs = await page.query_selector_all(\"article[data-testid='job-card']\")\n",
    "            print(f\"üìÑ Found {len(jobs)} job listings on page {page_num}\")\n",
    "\n",
    "            if len(jobs) < 32:\n",
    "                print(f\"üìâ Less than 32 jobs found ‚Äî likely last page. Stopping after this.\")\n",
    "                stop_scraping = True\n",
    "\n",
    "            for i in range(len(jobs)):\n",
    "                try:\n",
    "                    print(f\"üëâ Clicking job {i+1} on page {page_num}...\")\n",
    "\n",
    "                    jobs = await page.query_selector_all(\"article[data-testid='job-card']\")\n",
    "                    job = jobs[i]\n",
    "\n",
    "                    await job.click()\n",
    "                    await asyncio.sleep(2)\n",
    "\n",
    "                    try:\n",
    "                        await page.wait_for_selector(\"div[data-automation='jobAdDetails']\", timeout=10000)\n",
    "                    except:\n",
    "                        print(f\"‚ö†Ô∏è Skipping job {i+1}: job details panel failed to load.\")\n",
    "                        continue\n",
    "\n",
    "                    anchor = await job.query_selector(\"a[data-automation='job-list-view-job-link']\")\n",
    "                    url_suffix = await anchor.get_attribute(\"href\") if anchor else \"\"\n",
    "                    match = re.search(r\"/job/(\\d+)\", url_suffix)\n",
    "                    job_id = match.group(1) if match else \"N/A\"\n",
    "\n",
    "                    if job_id in existing_ids:\n",
    "                        print(f\"üõë Duplicate job ID '{job_id}' found ‚Äî stopping.\")\n",
    "                        stop_scraping = True\n",
    "                        break\n",
    "\n",
    "                    job_title_el = await page.query_selector(\"h1[data-automation='job-detail-title']\")\n",
    "                    job_title = await job_title_el.inner_text() if job_title_el else \"N/A\"\n",
    "\n",
    "                    company_el = await page.query_selector(\"span[data-automation='advertiser-name']\")\n",
    "                    company = await company_el.inner_text() if company_el else \"N/A\"\n",
    "\n",
    "                    location_el = await page.query_selector(\"span[data-automation='job-detail-location']\")\n",
    "                    location = await location_el.inner_text() if location_el else \"N/A\"\n",
    "\n",
    "                    work_type_el = await page.query_selector(\"span[data-automation='job-detail-work-type']\")\n",
    "                    work_type = await work_type_el.inner_text() if work_type_el else \"N/A\"\n",
    "\n",
    "                    classification_el = await page.query_selector(\"span[data-automation='job-detail-classifications']\")\n",
    "                    classification = await classification_el.inner_text() if classification_el else \"N/A\"\n",
    "\n",
    "                    salary_el = await page.query_selector(\"span[data-automation='job-detail-salary']\")\n",
    "                    salary = await salary_el.inner_text() if salary_el else \"\"\n",
    "\n",
    "                    desc_el = await page.query_selector(\"div[data-automation='jobAdDetails']\")\n",
    "                    job_description = await desc_el.inner_text() if desc_el else \"N/A\"\n",
    "\n",
    "                    posted_el = await job.query_selector(\"span[data-automation='jobListingDate']\")\n",
    "                    posted_raw = await posted_el.inner_text() if posted_el else \"\"\n",
    "                    posted_datetime = \"N/A\"\n",
    "                    try:\n",
    "                        if \"m\" in posted_raw:\n",
    "                            minutes = int(posted_raw.split(\"m\")[0].strip())\n",
    "                            posted_time = scrape_start_time - timedelta(minutes=minutes)\n",
    "                        elif \"hr\" in posted_raw:\n",
    "                            hours = int(posted_raw.split(\"hr\")[0].strip())\n",
    "                            posted_time = scrape_start_time - timedelta(hours=hours)\n",
    "                        elif \"d\" in posted_raw:\n",
    "                            days = int(posted_raw.split(\"d\")[0].strip())\n",
    "                            posted_time = scrape_start_time - timedelta(days=days)\n",
    "                        else:\n",
    "                            posted_time = scrape_start_time\n",
    "                        posted_datetime = posted_time.strftime(\"%d/%m/%y %H:%M\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Failed to parse posted time '{posted_raw}': {e}\")\n",
    "\n",
    "                    new_data.append({\n",
    "                        \"Job Title\": clean_text(job_title),\n",
    "                        \"Company\": clean_text(company),\n",
    "                        \"Location\": clean_text(location),\n",
    "                        \"Work Type\": clean_text(work_type),\n",
    "                        \"Classification\": clean_text(classification),\n",
    "                        \"Salary\": clean_text(salary),\n",
    "                        \"Job Description\": clean_text(job_description),\n",
    "                        \"Job ID\": clean_text(job_id),\n",
    "                        \"Posted Time\": clean_text(posted_datetime)\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error on job {i+1} of page {page_num}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            page_num += 1\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    if new_data:\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        full_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "        full_df.to_excel(file_path, index=False)\n",
    "        print(f\"\\n‚úÖ Scraping finished. {len(new_data)} new job(s) saved to Excel.\")\n",
    "    else:\n",
    "        print(\"\\nüìÇ No new jobs found to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44c5a0-82e2-4b41-963b-342e161c37c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåê Navigating to Page 1 ‚Äî https://ph.jobstreet.com/Data-Science-jobs?page=1&sortmode=ListedDate\n",
      "üìÑ Found 32 job listings on page 1\n",
      "üëâ Clicking job 1 on page 1...\n"
     ]
    }
   ],
   "source": [
    "# Run the async function inside Jupyter\n",
    "run = await scrape_jobstreet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04e6b9-b92a-4a5a-bdb8-8e7a9773c053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
